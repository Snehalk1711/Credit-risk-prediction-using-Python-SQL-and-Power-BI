# -*- coding: utf-8 -*-
"""Credit Risk.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1625pgOKk95Sxpat6P3C0dj9tQGPgn0Tr

Import libraries & dataset
"""

import pandas as pd
import numpy as np

# Load dataset
df = pd.read_csv("data_1.csv")

# Quick check
print(df.shape)
print(df.head())

"""Handle missing values

person_emp_length → ~895 nulls

loan_int_rate → ~3116 nulls
"""

# Fill missing employment length with median
df['person_emp_length'] = df['person_emp_length'].fillna(df['person_emp_length'].median())

# Fill missing loan interest rate with mean by loan grade
df['loan_int_rate'] = df.groupby('loan_grade')['loan_int_rate'].transform(lambda x: x.fillna(x.mean()))

"""Encode categorical variables

"""

from sklearn.preprocessing import LabelEncoder

cat_cols = ['person_home_ownership', 'loan_intent', 'loan_grade', 'cb_person_default_on_file']
le = LabelEncoder()

for col in cat_cols:
    df[col] = le.fit_transform(df[col])

"""# Task
Split the data into training and testing sets, train a model, and evaluate its performance.

## Split the data

### Subtask:
Split the data into training and testing sets.

**Reasoning**:
Split the data into training and testing sets using train_test_split.
"""

from sklearn.model_selection import train_test_split

X = df.drop('loan_status', axis=1)
y = df['loan_status']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

"""## Train a model

### Subtask:
Choose and train a suitable model for the credit risk prediction task.

**Reasoning**:
Import Logistic Regression, instantiate the model, and train it using the training data.
"""

from sklearn.linear_model import LogisticRegression

model = LogisticRegression()
model.fit(X_train, y_train)

"""## Evaluate the model

### Subtask:
Evaluate the performance of the trained model using appropriate metrics.

**Reasoning**:
Evaluate the performance of the trained model using appropriate metrics.
"""

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

y_pred = model.predict(X_test)

accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

print(f"Accuracy: {accuracy:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1-score: {f1:.4f}")

"""## Summary:

### Data Analysis Key Findings

*   The data was successfully split into training and testing sets with a test size of 20%.
*   A Logistic Regression model was trained on the training data.
*   The model achieved an accuracy of 0.8206 on the test set.
*   The precision of the model was 0.6840, recall was 0.3550, and the F1-score was 0.4674.

### Insights or Next Steps

*   The relatively low recall score suggests the model is not effectively identifying all positive cases. Further investigation into model tuning or alternative algorithms may be beneficial.
*   Address the `ConvergenceWarning` during model training, potentially by scaling the data or increasing the maximum number of iterations.

Build Models

1. Logistic Regression (baseline)
"""

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, roc_auc_score

log_model = LogisticRegression(max_iter=1000)
log_model.fit(X_train, y_train)

y_pred = log_model.predict(X_test)
print(classification_report(y_test, y_pred))
print("ROC-AUC:", roc_auc_score(y_test, log_model.predict_proba(X_test)[:,1]))

"""2. Random Forest (stronger model)"""

from sklearn.ensemble import RandomForestClassifier

rf_model = RandomForestClassifier(n_estimators=200, random_state=42)
rf_model.fit(X_train, y_train)

y_pred_rf = rf_model.predict(X_test)
print(classification_report(y_test, y_pred_rf))
print("ROC-AUC:", roc_auc_score(y_test, rf_model.predict_proba(X_test)[:,1]))

"""Feature Importance"""

import matplotlib.pyplot as plt

feat_importances = pd.Series(rf_model.feature_importances_, index=X.columns)
feat_importances.nlargest(10).plot(kind='barh')
plt.title("Top 10 Important Features")
plt.show()

"""Outlier Detection & Handling (Quick Check)"""

import numpy as np

# Make a copy of df
df_clean = df.copy()

# Capping extreme values (Winsorization style)
df_clean["person_age"] = np.where(df_clean["person_age"] > 90, 90, df_clean["person_age"])
df_clean["person_income"] = np.where(df_clean["person_income"] > 200000, 200000, df_clean["person_income"])
df_clean["person_emp_length"] = np.where(df_clean["person_emp_length"] > 50, 50, df_clean["person_emp_length"])
df_clean["loan_amnt"] = np.where(df_clean["loan_amnt"] > 40000, 40000, df_clean["loan_amnt"])
df_clean["loan_int_rate"] = np.where(df_clean["loan_int_rate"] > 30, 30, df_clean["loan_int_rate"])
df_clean["loan_percent_income"] = np.where(df_clean["loan_percent_income"] > 1, 1, df_clean["loan_percent_income"])
df_clean["cb_person_cred_hist_length"] = np.where(df_clean["cb_person_cred_hist_length"] > 30, 30, df_clean["cb_person_cred_hist_length"])

"""Model Evaluation: Confusion Matrix + ROC"""

from sklearn.metrics import confusion_matrix, roc_curve, auc, ConfusionMatrixDisplay

# Predictions (assuming X_test, y_test already exist)
y_pred = rf_model.predict(X_test)
y_prob = rf_model.predict_proba(X_test)[:,1]

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=["Paid", "Default"])
disp.plot(cmap="Blues")
plt.title("Confusion Matrix - Random Forest")
plt.show()

# ROC Curve
fpr, tpr, thresholds = roc_curve(y_test, y_prob)
roc_auc = auc(fpr, tpr)

plt.figure()
plt.plot(fpr, tpr, color="darkorange", lw=2, label="ROC curve (AUC = %0.2f)" % roc_auc)
plt.plot([0, 1], [0, 1], color="navy", lw=2, linestyle="--")
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve - Random Forest")
plt.legend(loc="lower right")
plt.show()

"""Visualization for Dashboard Prep"""

import seaborn as sns
import matplotlib.pyplot as plt

# Loan Status Distribution
sns.countplot(x="loan_status", data=df)
plt.title("Loan Status Distribution (0 = Paid, 1 = Default)")
plt.show()

# Loan Grade vs Interest Rate
sns.boxplot(x="loan_grade", y="loan_int_rate", data=df)
plt.title("Interest Rate vs Loan Grade")
plt.show()

# Income vs Loan Status
sns.boxplot(x="loan_status", y="person_income", data=df)
plt.ylim(0, 200000)  # limit scale if extreme outliers
plt.title("Income vs Loan Status")
plt.show()

# Loan Percent Income vs Loan Status
sns.boxplot(x="loan_status", y="loan_percent_income", data=df)
plt.title("Loan % of Income vs Loan Status")
plt.show()

"""Feature Engineering (Bins for Dashboard)"""

# Age Groups
df_clean["age_group"] = pd.cut(
    df_clean["person_age"],
    bins=[0, 25, 35, 50, 90],
    labels=["18-25", "26-35", "36-50", "50+"]
)

# Income Brackets
df_clean["income_bracket"] = pd.cut(
    df_clean["person_income"],
    bins=[0, 30000, 70000, 200000],
    labels=["Low", "Medium", "High"]
)

# Loan Amount Ranges
df_clean["loan_amnt_range"] = pd.cut(
    df_clean["loan_amnt"],
    bins=[0, 10000, 20000, 40000],
    labels=["Small", "Medium", "Large"]
)

"""Export Clean Data for Power BI"""

# Export clean dataset
df_clean.to_csv("credit_risk_clean.csv", index=False)
print("✅ Clean dataset exported as credit_risk_clean.csv")
